{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"17cmQ2DXwjldU4v3rrXoWNXTd01YNOwi3","authorship_tag":"ABX9TyNaA6Y4HQ6ZHp/ZOoN3FOq/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnkfMZ9cHS5h","executionInfo":{"status":"ok","timestamp":1760693471248,"user_tz":420,"elapsed":5227,"user":{"displayName":"swaroop kolasani","userId":"00185389200961531281"}},"outputId":"3eee93cb-b94e-4cf3-c213-48de1e5ee0ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/Othercomputers/My_Mac/sentinel')\n","\n","import os\n","os.chdir('/content/drive/Othercomputers/My_Mac/sentinel')\n","\n","!pip install -q torch torchvision tensorboard google-cloud-storage tqdm\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","import numpy as np\n","import os\n","from datetime import datetime\n","\n","# Custom modules\n","from src.python.models.pointnet2 import PointNet2SemanticSegmentation\n","from src.python.datasets.semantic_kitti import SemanticKITTIDataset\n","from src.python.datasets.data_augmentation import PointCloudAugmentation\n","from src.python.config.training_config import TrainingConfig\n","from src.python.utils.metrics import calculate_iou, calculate_accuracy\n"],"metadata":{"id":"EywXQxPzHm0v","executionInfo":{"status":"ok","timestamp":1760693477402,"user_tz":420,"elapsed":6150,"user":{"displayName":"swaroop kolasani","userId":"00185389200961531281"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["config = TrainingConfig()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","os.makedirs(config.get('paths.checkpoint_dir'), exist_ok=True)\n","os.makedirs(config.get('paths.log_dir'), exist_ok=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LT_SxTfPIPUn","executionInfo":{"status":"ok","timestamp":1760693477416,"user_tz":420,"elapsed":7,"user":{"displayName":"swaroop kolasani","userId":"00185389200961531281"}},"outputId":"7a122c98-8aa8-4483-e20f-4d4967a19cce"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["from google.colab import auth\n","auth.authenticate_user()\n","train_dataset = SemanticKITTIDataset(\n","    root_dir='',\n","    split='train',\n","    use_gcs=True,\n","    bucket_name=config.get('gcs.bucket_name')\n",")\n","\n","val_dataset = SemanticKITTIDataset(\n","    root_dir='',\n","    split='val',\n","    use_gcs=True,\n","    bucket_name=config.get('gcs.bucket_name')\n",")\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=config.get('training.batch_size'),\n","    shuffle=True,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=config.get('training.batch_size'),\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","print(f\"Train dataset size: {len(train_dataset)}\")\n","print(f\"Val dataset size: {len(val_dataset)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tNC3FyBImKz","executionInfo":{"status":"ok","timestamp":1760693482230,"user_tz":420,"elapsed":4812,"user":{"displayName":"swaroop kolasani","userId":"00185389200961531281"}},"outputId":"e803286e-fbfc-4e5f-9073-30cf929a54c0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset size: 19130\n","Val dataset size: 4071\n"]}]},{"cell_type":"code","source":["model = PointNet2SemanticSegmentation(\n","    num_classes=config.get('model.num_classes')\n",").to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(\n","    model.parameters(),\n","    lr=config.get('training.learning_rate'),\n","    weight_decay=config.get('training.weight_decay')\n",")\n","\n","scheduler = optim.lr_scheduler.StepLR(\n","    optimizer,\n","    step_size=config.get('training.scheduler.step_size'),\n","    gamma=config.get('training.scheduler.gamma')\n",")\n","\n","writer = SummaryWriter(config.get('paths.log_dir'))\n","\n","print(\"Model initialized successfully\")\n","print(f\"Total parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NAqDg5BtJnGt","executionInfo":{"status":"ok","timestamp":1760693483395,"user_tz":420,"elapsed":1166,"user":{"displayName":"swaroop kolasani","userId":"00185389200961531281"}},"outputId":"f2d8975f-f028-40df-b731-442d8c88a209"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model initialized successfully\n","Total parameters: 0.97M\n"]}]},{"cell_type":"code","source":["def train_epoch(model, dataloader, criterion, optimizer, device, epoch, writer):\n","    model.train()\n","    total_loss = 0\n","    total_correct = 0\n","    total_points = 0\n","\n","    pbar = tqdm(dataloader, desc=f\"Training Epoch {epoch}\")\n","    for batch_idx, (points, labels) in enumerate(pbar):\n","        points = points.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        predictions = model(points)\n","\n","        # Reshape for loss calculation\n","        loss = criterion(\n","            predictions.reshape(-1, predictions.shape[-1]),\n","            labels.reshape(-1)\n","        )\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Accuracy calculation\n","        pred_labels = predictions.argmax(dim=-1)\n","        total_correct += (pred_labels == labels).sum().item()\n","        total_points += labels.numel()\n","        total_loss += loss.item()\n","\n","        # Logging\n","        global_step = epoch * len(dataloader) + batch_idx\n","        writer.add_scalar('Train/Loss', loss.item(), global_step)\n","\n","        pbar.set_postfix({\n","            'loss': f\"{loss.item():.4f}\",\n","            'acc': f\"{100 * total_correct / total_points:.2f}%\"\n","        })\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = total_correct / total_points\n","    return avg_loss, accuracy\n","def validate(model, dataloader, criterion, device, epoch, writer):\n","    model.eval()\n","    total_loss = 0\n","    total_correct = 0\n","    total_points = 0\n","\n","    with torch.no_grad():\n","        pbar = tqdm(dataloader, desc=f\"Validation\")\n","        for points, labels in pbar:\n","            points = points.to(device)\n","            labels = labels.to(device)\n","\n","            predictions = model(points)\n","\n","            loss = criterion(\n","                predictions.reshape(-1, predictions.shape[-1]),\n","                labels.reshape(-1)\n","            )\n","\n","            pred_labels = predictions.argmax(dim=-1)\n","            total_correct += (pred_labels == labels).sum().item()\n","            total_points += labels.numel()\n","            total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = total_correct / total_points\n","    writer.add_scalar('Val/Loss', avg_loss, epoch)\n","    writer.add_scalar('Val/Accuracy', accuracy, epoch)\n","    return avg_loss, accuracy\n"],"metadata":{"id":"M9dBsNrhJ6XC","executionInfo":{"status":"ok","timestamp":1760693483429,"user_tz":420,"elapsed":3,"user":{"displayName":"swaroop kolasani","userId":"00185389200961531281"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, dataloader, criterion, optimizer, device, epoch, writer):\n","    model.train()\n","    total_loss = 0\n","    total_correct = 0\n","    total_points = 0\n","\n","    pbar = tqdm(dataloader, desc=f\"Training Epoch {epoch}\")\n","    for batch_idx, (points, labels) in enumerate(pbar):\n","        points = points.to(device)  # [B, N, 4]\n","        labels = labels.to(device)  # [B, N]\n","\n","        # Forward pass\n","        optimizer.zero_grad()\n","        predictions = model(points)  # [B, N, num_classes]\n","\n","        # Reshape for loss calculation\n","        # predictions: [B, N, num_classes] -> [B*N, num_classes]\n","        # labels: [B, N] -> [B*N]\n","        loss = criterion(\n","            predictions.reshape(-1, predictions.shape[-1]),\n","            labels.reshape(-1)\n","        )\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Statistics\n","        total_loss += loss.item()\n","        pred_labels = predictions.argmax(dim=-1)  # [B, N]\n","        total_correct += (pred_labels == labels).sum().item()\n","        total_points += labels.numel()\n","\n","        # Update progress bar\n","        pbar.set_postfix({\n","            'loss': f\"{loss.item():.4f}\",\n","            'acc': f\"{100*total_correct/total_points:.2f}%\"\n","        })\n","\n","        # Log to tensorboard\n","        global_step = epoch * len(dataloader) + batch_idx\n","        writer.add_scalar('Train/Loss', loss.item(), global_step)\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = total_correct / total_points\n","\n","    return avg_loss, accuracy"],"metadata":{"id":"I-ynTCjPfU0I","executionInfo":{"status":"ok","timestamp":1760693483430,"user_tz":420,"elapsed":0,"user":{"displayName":"swaroop kolasani","userId":"00185389200961531281"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","num_epochs = config.get('training.epochs')\n","best_val_acc = 0\n","\n","for epoch in range(1, num_epochs + 1):\n","    print(f\"\\n{'='*50}\")\n","    print(f\"Epoch {epoch}/{num_epochs}\")\n","    print(f\"{'='*50}\")\n","\n","    # Training\n","    train_loss, train_acc = train_epoch(\n","        model, train_loader, criterion, optimizer, device, epoch, writer\n","    )\n","\n","    # Validation\n","    val_loss, val_acc = validate(\n","        model, val_loader, criterion, device, epoch, writer\n","    )\n","\n","    # Update learning rate\n","    scheduler.step()\n","\n","    # Print epoch results\n","    print(f\"\\nEpoch {epoch} Results:\")\n","    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n","    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n","\n","    # Save checkpoint\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'train_loss': train_loss,\n","        'val_loss': val_loss,\n","        'train_acc': train_acc,\n","        'val_acc': val_acc,\n","    }\n","\n","    # Save regular checkpoint\n","    checkpoint_path = os.path.join(\n","        config.get('paths.checkpoint_dir'),\n","        f'checkpoint_epoch_{epoch}.pth'\n","    )\n","    torch.save(checkpoint, checkpoint_path)\n","\n","    # Save best model\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        best_path = os.path.join(\n","            config.get('paths.checkpoint_dir'),\n","            'best_model.pth'\n","        )\n","        torch.save(checkpoint, best_path)\n","        print(f\"  ✓ New best model saved! Val Acc: {val_acc:.4f}\")\n","\n","print(\"\\n\" + \"=\"*50)\n","print(\"Training Complete!\")\n","print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n","print(\"=\"*50)\n","\n","writer.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FBFItfRJ8a-","outputId":"efe0dbd4-9792-4bb5-fc15-c9dddd9457d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","Epoch 1/100\n","==================================================\n"]},{"output_type":"stream","name":"stderr","text":["Training Epoch 1:   7%|▋         | 645/9565 [06:54<1:30:51,  1.64it/s, loss=1.2277, acc=49.28%]"]}]}]}